{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Gemma-1.1-2b-it for Code Reasoning with `nvidia/OpenCodeReasoning`\n",
    "\n",
    "This notebook demonstrates how to fine-tune the `google/gemma-1.1-2b-it` model on the `nvidia/OpenCodeReasoning` dataset using Hugging Face libraries (`transformers`, `datasets`, `peft`, `trl`).\n",
    "\n",
    "**Goal:** To adapt Gemma to better understand and generate code or reason about code based on the provided dataset.\n",
    "\n",
    "**Steps Covered:**\n",
    "1.  **Install Libraries:** Set up the environment with necessary packages.\n",
    "2.  **Load and Preprocess Dataset:** Load `nvidia/OpenCodeReasoning`, explore it, and prepare it for training. This step is CRITICAL and requires careful adaptation to the dataset's specific structure and Gemma's required input format.\n",
    "3.  **Load Gemma Model and Tokenizer:** Load the base `gemma-1.1-2b-it` model with 4-bit quantization for efficiency.\n",
    "4.  **Configure Fine-Tuning:** Set up LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning and define training arguments.\n",
    "5.  **Run Fine-Tuning:** Train the model using `SFTTrainer`.\n",
    "6.  **Save Model:** Save the trained LoRA adapter.\n",
    "7.  **Inference:** Test the fine-tuned model with sample prompts.\n",
    "\n",
    "**Important Considerations for `nvidia/OpenCodeReasoning`:**\n",
    "*   **Dataset Structure:** You **MUST** inspect the `nvidia/OpenCodeReasoning` dataset to understand its columns (e.g., prompt, code, reasoning, etc.). The `preprocess_function` (Section 2.3) and the inference prompt format (Section 7.1) need to be tailored to this structure.\n",
    "*   **Prompt Engineering:** The way you format your input to Gemma is crucial. For instruction-tuned models like `gemma-1.1-2b-it`, you need to use its specific chat/instruction template (e.g., involving `<start_of_turn>user`, `<end_of_turn>`, `<start_of_turn>model`). The placeholder examples in this notebook will need to be updated.\n",
    "*   **Colab Resources:** Fine-tuning can be resource-intensive. This notebook uses 4-bit quantization and LoRA to make it more feasible on platforms like Google Colab with a T4 GPU. You might need to adjust batch sizes or other parameters based on available memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate bitsandbytes peft trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load Dataset\n",
    "We'll load the `nvidia/OpenCodeReasoning` dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"nvidia/OpenCodeReasoning\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\") # You might want to specify a split or use different splits\n",
    "\n",
    "# Let's look at an example from the dataset\n",
    "print(dataset[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Explore Dataset (Optional)\n",
    "It's good practice to understand the structure of your data. You can print out more examples, check column names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Print dataset features\n",
    "print(dataset.features)\n",
    "\n",
    "# Example: Show a few more examples\n",
    "for i in range(1, 4):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Preprocess and Tokenize\n",
    "We need to tokenize the text data and format it into input/output pairs that the model can learn from.\n",
    "The exact preprocessing will depend on the dataset structure and the task. \n",
    "For Gemma, we typically need to format the input as a chat/instruction.\n",
    "\n",
    "**Note:** This is a generic preprocessing step. You'll likely need to adapt this based on the specific columns and structure of the `nvidia/OpenCodeReasoning` dataset.\n",
    "You might need to combine columns or reformat them to create a clear prompt and response structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer for Gemma\n",
    "# Make sure to use the specific Gemma model you intend to fine-tune (e.g., \"google/gemma-1.1-2b-it\")\n",
    "model_id = \"google/gemma-1.1-2b-it\" # Updated model_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Add a padding token if it doesn't exist. Gemma models might not have one by default.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'}) \n",
    "    # Important: If you add a pad token, you might need to resize model token embeddings later\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    # This is a placeholder. You MUST adapt this to your dataset.\n",
    "    # For example, if your dataset has 'prompt' and 'completion' columns:\n",
    "    # inputs = [prompt for prompt in examples['prompt']]\n",
    "    # outputs = [completion for completion in examples['completion']]\n",
    "    # text = [f\"Prompt: {inp} \\nResponse: {out}{tokenizer.eos_token}\" for inp, out in zip(inputs, outputs)]\n",
    "    \n",
    "    # Example: Assuming 'description' is the input and 'code' is the output for OpenCodeReasoning\n",
    "    # This is a guess and needs to be verified with the dataset's actual structure.\n",
    "    # You might need to format it like an instruction or a question.\n",
    "    # For instance: \"Translate the following description to code: [DESCRIPTION] \\n\\n [CODE]\"\n",
    "\n",
    "    # Let's assume the dataset has a 'prompt' and 'solution' field for now.\n",
    "    # This needs to be verified and adapted based on nvidia/OpenCodeReasoning structure.\n",
    "    if 'prompt' in examples and 'solution' in examples:\n",
    "        text = [f\"Instruction: {p}\\nOutput: {s}{tokenizer.eos_token}\" for p, s in zip(examples['prompt'], examples['solution'])]\n",
    "    elif 'description' in examples and 'code_string' in examples: # Another guess based on common code dataset structures\n",
    "        text = [f\"Generate code for the following description:\\n{d}\\n\\nCode:\\n{c}{tokenizer.eos_token}\" for d, c in zip(examples['description'], examples['code_string'])]\n",
    "    else:\n",
    "        # Fallback: just take the first text column if the above are not found.\n",
    "        # THIS WILL LIKELY NOT WORK WELL AND NEEDS ADJUSTMENT.\n",
    "        first_text_column = [key for key, value in examples.items() if isinstance(examples[key][0], str)][0]\n",
    "        print(f\"Warning: Using generic column '{first_text_column}' for tokenization. Please adapt the preprocess_function for your dataset structure.\")\n",
    "        text = [t + tokenizer.eos_token for t in examples[first_text_column]]\n",
    "\n",
    "    return tokenizer(text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "\n",
    "# Apply the preprocessing function\n",
    "# This might take a while\n",
    "# Consider using a subset for faster iteration initially: small_dataset = dataset.select(range(1000))\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "print(\"Example of tokenized data:\")\n",
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load Gemma Model and Tokenizer\n",
    "We'll load the Gemma model (e.g., `google/gemma-1.1-2b-it`) and its tokenizer. We'll also configure it for 4-bit quantization to save memory, which is crucial for running larger models in environments like Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Model ID for Gemma\n",
    "# User requested gemma-3-1b. The closest available instruction-tuned model of a smaller size is gemma-1.1-2b-it.\n",
    "# If a specific \"3-1b\" variant becomes available and is preferred, this ID should be updated.\n",
    "# For now, we use \"google/gemma-1.1-2b-it\" as a robust choice for fine-tuning.\n",
    "model_id = \"google/gemma-1.1-2b-it\" \n",
    "\n",
    "# Configure BitsAndBytes for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 for compute if available, otherwise float16\n",
    ")\n",
    "\n",
    "# Load the model with quantization config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\", # Automatically map model parts to available devices (CPU/GPU)\n",
    "    # token=\"YOUR_HF_TOKEN\" # Add this if you encounter auth issues for gated models like some Gemma versions\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "# The tokenizer was already loaded in the preprocessing section.\n",
    "# We can reload it here or ensure the one from preprocessing is correctly configured.\n",
    "# For consistency and to ensure it's available in this section's scope:\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set pad token if not set. Gemma tokenizers might not have a pad token by default.\n",
    "# Using EOS token as pad token is a common strategy if a dedicated pad token is missing.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Important: If you set pad_token to eos_token, you must ensure your model's config reflects this,\n",
    "    # especially for generation. Some models might require resizing token embeddings if a new token is added.\n",
    "    # For fine-tuning with SFTTrainer, this is generally handled, but good to be aware of.\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "# Resize token embeddings if a new pad token was added (e.g. [PAD]) \n",
    "# and it's different from existing special tokens.\n",
    "# This step was mentioned in the previous section if we added a *new* special token like '[PAD]'.\n",
    "# If we just set pad_token = eos_token, resizing is usually not needed unless eos_token was somehow not in the embeddings initially.\n",
    "# For now, assuming the previous tokenizer setup handled any necessary additions.\n",
    "# if tokenizer.pad_token == '[PAD]': # Only if we added a new '[PAD]' token\n",
    "#    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "print(f\"Model '{model_id}' loaded with 4-bit quantization.\")\n",
    "print(f\"Tokenizer '{model_id}' loaded.\")\n",
    "if tokenizer.pad_token:\n",
    "    print(f\"Pad token is set to: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "else:\n",
    "    print(\"Warning: Pad token is not set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Verify Tokenizer and Model Alignment (Important for Gemma)\n",
    "Gemma models, especially instruction-tuned ones, have specific ways they expect prompts to be formatted, often using special tokens like `<start_of_turn>` and `<end_of_turn>`. \n",
    "The tokenizer handles adding these for chat templates, but when constructing strings manually (as in the `preprocess_function`), you need to be mindful.\n",
    "The `SFTTrainer` often expects a 'text' field in the dataset that contains the fully formatted prompt including any special tokens.\n",
    "\n",
    "The previous preprocessing step should create a single string field (e.g., `text`) in your dataset.\n",
    "Ensure that this string is formatted correctly for Gemma.\n",
    "For `gemma-1.1-2b-it`, a typical instruction format might be:\n",
    "`<start_of_turn>user\n{your_instruction_or_prompt}<end_of_turn>\n<start_of_turn>model\n{your_expected_response}<end_of_turn>`\n",
    "\n",
    "The `preprocess_function` needs to be adapted to create this structure using the columns from `nvidia/OpenCodeReasoning`.\n",
    "The current placeholder in `preprocess_function` (e.g., `f\"Instruction: {p}\\nOutput: {s}{tokenizer.eos_token}\"`) is a simplification and should be updated to match Gemma's required format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Configure Fine-Tuning\n",
    "We'll set up the training arguments and LoRA (Low-Rank Adaptation) configuration for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. LoRA Configuration (PEFT)\n",
    "PEFT (Parameter-Efficient Fine-Tuning) methods like LoRA allow us to fine-tune large models by training only a small number of extra parameters, significantly reducing computational and memory costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Before applying PEFT, if you're using a quantized model (like with BitsAndBytesConfig),\n",
    "# it's often recommended to prepare it for k-bit training.\n",
    "# This function can help handle some of the intricacies.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of the LoRA matrices. Higher rank means more parameters, potentially better performance but more memory.\n",
    "    lora_alpha=32,  # Alpha scaling factor. alpha/r controls the scaling of LoRA weights.\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Modules to apply LoRA to. These are typical for Gemma.\n",
    "    lora_dropout=0.05,  # Dropout probability for LoRA layers.\n",
    "    bias=\"none\",  # Whether to train bias parameters. 'none' is common.\n",
    "    task_type=\"CAUSAL_LM\", # Task type, Causal Language Modeling for Gemma.\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print a summary of trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Training Arguments\n",
    "These arguments control various aspects of the training process, such as learning rate, batch size, number of epochs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define output directory for saving checkpoints and final model\n",
    "output_dir = \"./gemma_opencodereasoning_finetuned\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1, # Start with 1 epoch, can be increased. For large datasets, even a fraction of an epoch might be enough.\n",
    "    per_device_train_batch_size=4, # Adjust based on your GPU memory. 4 is a common starting point for 4-bit models.\n",
    "    gradient_accumulation_steps=2, # Accumulate gradients over multiple steps to simulate a larger batch size.\n",
    "    optim=\"paged_adamw_8bit\", # Optimizer that works well with quantized models.\n",
    "    save_steps=100, # Save a checkpoint every N steps.\n",
    "    logging_steps=10, # Log training metrics every N steps.\n",
    "    learning_rate=2e-4, # A common learning rate for LoRA fine-tuning.\n",
    "    weight_decay=0.001, # Weight decay for regularization.\n",
    "    fp16=False, # Set to True if your GPU supports FP16 and you're not using bfloat16 compute in BitsAndBytes.\n",
    "    bf16=True, # Set to True if your GPU supports BF16 (recommended for Ampere and newer GPUs) and using bfloat16 compute.\n",
    "    max_grad_norm=0.3, # Gradient clipping to prevent exploding gradients.\n",
    "    max_steps=-1, # If set to a positive number, overrides num_train_epochs.\n",
    "    warmup_ratio=0.03, # Ratio of total training steps for learning rate warmup.\n",
    "    group_by_length=True, # Group sequences of similar lengths together to optimize padding and reduce training time.\n",
    "    lr_scheduler_type=\"constant_with_warmup\", # Learning rate scheduler.\n",
    "    report_to=\"tensorboard\", # Or \"wandb\", \"none\", etc.\n",
    "    # packing=True, # If using SFTTrainer and your dataset is already preprocessed with packing, set this to True.\n",
    "                    # Requires dataset to have 'input_ids', 'attention_mask', 'labels' from packing.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run Fine-Tuning\n",
    "Now we initialize the `SFTTrainer` from TRL (Transformer Reinforcement Learning) library, which is designed for supervised fine-tuning of language models, and start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Ensure the tokenized dataset is used here.\n",
    "# The SFTTrainer expects a 'text' field by default, or you can specify `dataset_text_field`.\n",
    "# Our `preprocess_function` should have created a tokenized dataset where each item\n",
    "# is a dictionary of 'input_ids', 'attention_mask', etc.\n",
    "# SFTTrainer can handle this directly. If the preprocess_function created a 'text' column\n",
    "# that was then tokenized and that 'text' column was removed, that's fine.\n",
    "# The trainer will use the columns 'input_ids', 'attention_mask', 'labels'.\n",
    "\n",
    "# If your `preprocess_function` did not create a single text field that was then tokenized,\n",
    "# but rather directly outputted tokenized 'input_ids', 'attention_mask', 'labels',\n",
    "# you might need to specify `dataset_text_field=None` or ensure the dataset is correctly formatted.\n",
    "# Given our current `preprocess_function` tokenizes a constructed text string,\n",
    "# we should ensure the `tokenized_dataset` is what we pass.\n",
    "# The `SFTTrainer` is flexible; it can take a raw text dataset and a tokenizer,\n",
    "# or a pre-tokenized dataset.\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                             # The PEFT-prepared model\n",
    "    train_dataset=tokenized_dataset,         # The tokenized training dataset\n",
    "    # eval_dataset=tokenized_eval_dataset,   # Optionally, pass an evaluation dataset\n",
    "    peft_config=lora_config,                 # The LoRA configuration\n",
    "    dataset_text_field=None,                 # Set to your text column name if you haven't pre-tokenized and want trainer to tokenize.\n",
    "                                             # If dataset is already tokenized (input_ids, attention_mask), set to None or don't specify.\n",
    "                                             # Our current `tokenized_dataset` is already tokenized.\n",
    "    tokenizer=tokenizer,                     # The tokenizer\n",
    "    args=training_args,                      # The training arguments\n",
    "    max_seq_length=512,                      # Max sequence length for packing (if `packing=True` in TrainingArguments) or truncation.\n",
    "                                             # Ensure this matches the `max_length` in `preprocess_function` if not packing.\n",
    "    # packing=True,                          # Set to True if your dataset is prepared for packing. This can speed up training.\n",
    "                                             # If True, dataset needs 'input_ids', 'attention_mask', 'labels' for each packed sequence.\n",
    "                                             # Our current `preprocess_function` does not implement packing.\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. (Important) Adapting `preprocess_function` for `SFTTrainer`\n",
    "The `SFTTrainer` works best if the dataset contains a single text field that represents the full conversation turn or instruction-response pair, already formatted with any special tokens (like Gemma's `<start_of_turn>`, `<end_of_turn>`).\n",
    "\n",
    "Our current `preprocess_function` creates a tokenized output directly.\n",
    "```python\n",
    "# Recall from section 2.3:\n",
    "# def preprocess_function(examples):\n",
    "#     # ...\n",
    "#     # text = [f\"Instruction: {p}\\nOutput: {s}{tokenizer.eos_token}\" for p, s in zip(examples['prompt'], examples['solution'])]\n",
    "#     # ...\n",
    "#     return tokenizer(text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "#\n",
    "# tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
    "```\n",
    "This approach is generally fine as `SFTTrainer` can handle pre-tokenized datasets. The `remove_columns=dataset.column_names` ensures that only `input_ids`, `attention_mask`, `labels` (implicitly created by the tokenizer for causal LM) are passed to the trainer.\n",
    "\n",
    "**Alternative for `SFTTrainer` (if you prefer it to handle tokenization):**\n",
    "You could modify `preprocess_function` to output a new column (e.g., `formatted_text`) and then specify `dataset_text_field=\"formatted_text\"` in `SFTTrainer`.\n",
    "\n",
    "Example modification:\n",
    "```python\n",
    "# def preprocess_for_sft(examples):\n",
    "#    # Adapt this based on nvidia/OpenCodeReasoning columns and Gemma format\n",
    "#    # e.g., using 'prompt' and 'solution'\n",
    "#    texts = []\n",
    "#    for p, s in zip(examples['prompt'], examples['solution']):\n",
    "#        # This formatting needs to be Gemma specific!\n",
    "#        # <start_of_turn>user\n",
    "{PROMPT}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{SOLUTION}<end_of_turn>\n",
    "#        # The tokenizer.apply_chat_template might be useful if your data fits a chat structure.\n",
    "#        # For now, a simplified example:\n",
    "#        formatted_prompt = f\"<start_of_turn>user\\n{p}<end_of_turn>\\n<start_of_turn>model\\n{s}{tokenizer.eos_token}\"\n",
    "#        texts.append(formatted_prompt)\n",
    "#    return {\"text\": texts} # SFTTrainer will look for this 'text' field\n",
    "\n",
    "# formatted_dataset = dataset.map(preprocess_for_sft, batched=True, remove_columns=dataset.column_names)\n",
    "# trainer = SFTTrainer(..., train_dataset=formatted_dataset, dataset_text_field=\"text\", tokenizer=tokenizer, ...)\n",
    "```\n",
    "For now, we will stick to providing the already tokenized dataset to the `SFTTrainer` as implemented.\n",
    "The key is that the `tokenized_dataset` contains `input_ids`, `attention_mask`, and `labels`. The `transformers.AutoTokenizer` when used for causal LM tasks and with labels (which it infers unless explicitly told not to) should prepare these correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Save the Fine-Tuned Model\n",
    "After training, we need to save the fine-tuned model. With PEFT (LoRA), we are primarily saving the adapter weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a path to save the LoRA adapter\n",
    "adapter_output_dir = f\"{output_dir}/final_adapter\" \n",
    "\n",
    "# Save the LoRA adapter\n",
    "trainer.save_model(adapter_output_dir)\n",
    "print(f\"LoRA adapter saved to: {adapter_output_dir}\")\n",
    "\n",
    "# Optionally, if you want to save the full model (merged with LoRA weights)\n",
    "# This will require more disk space and memory.\n",
    "# Make sure you have enough resources before uncommenting.\n",
    "\n",
    "# print(\"Merging adapter weights with the base model...\")\n",
    "# merged_model = model.merge_and_unload() # Merges LoRA weights and unloads PEFT model, returning the base model with merged weights.\n",
    "# print(\"Adapter weights merged.\")\n",
    "\n",
    "# Define a path to save the full merged model\n",
    "# merged_model_output_dir = f\"{output_dir}/final_merged_model\"\n",
    "# merged_model.save_pretrained(merged_model_output_dir)\n",
    "# tokenizer.save_pretrained(merged_model_output_dir)\n",
    "# print(f\"Full merged model saved to: {merged_model_output_dir}\")\n",
    "\n",
    "# For most use cases with LoRA, deploying the base model and loading the adapter separately is common.\n",
    "# However, merging can be useful for some deployment scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Basic Inference with Fine-Tuned Model\n",
    "Let's test the fine-tuned model with some sample prompts. We'll load the base model and then apply the saved LoRA adapter weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Ensure CUDA is available if you trained on GPU and want to infer on GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Option 1: Load the base model and then apply the LoRA adapter ---\n",
    "# This is the most common way when you've saved only the adapter.\n",
    "\n",
    "# Load the base model (the same one used for training)\n",
    "base_model_id = \"google/gemma-1.1-2b-it\" # Should be the same model_id used earlier\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Ensure pad token is set for generation\n",
    "\n",
    "# Load the base model in 4-bit (or your original configuration)\n",
    "# If you used 4-bit for training, use it for inference too for consistency,\n",
    "# unless you specifically want to test a different precision.\n",
    "bnb_config_inference = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config_inference,\n",
    "    device_map=\"auto\", # Automatically map model to available device(s)\n",
    ")\n",
    "if tokenizer.pad_token == tokenizer.eos_token: # Make sure model config reflects this\n",
    "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load the LoRA adapter\n",
    "# adapter_output_dir was defined in the saving section (e.g., \"./gemma_opencodereasoning_finetuned/final_adapter\")\n",
    "ft_model = PeftModel.from_pretrained(base_model, adapter_output_dir)\n",
    "ft_model = ft_model.to(device) # Ensure the PEFT model is on the correct device\n",
    "ft_model.eval() # Set the model to evaluation mode\n",
    "\n",
    "print(f\"Fine-tuned model (base + adapter) loaded from {adapter_output_dir} and ready for inference on {device}.\")\n",
    "\n",
    "# --- Option 2: If you saved a merged model ---\n",
    "# If you uncommented and ran the `merged_model.save_pretrained(...)` part earlier:\n",
    "# merged_model_path = f\"{output_dir}/final_merged_model\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
    "# ft_model = AutoModelForCausalLM.from_pretrained(merged_model_path, device_map=\"auto\")\n",
    "# ft_model.eval()\n",
    "# print(f\"Full fine-tuned model loaded from {merged_model_path} and ready for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Generate Text\n",
    "Now, let's create a prompt. **This prompt should be formatted in the same way as your training data examples.**\n",
    "For Gemma instruction-tuned models, this usually means a specific chat-like template.\n",
    "\n",
    "For example, if your training data was formatted like:\n",
    "`<start_of_turn>user\n{instruction}<end_of_turn>\n<start_of_turn>model\n{response}<end_of_turn>`\n",
    "\n",
    "Your prompt for inference should only contain the user part, up to `<start_of_turn>model\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt - **ADAPT THIS TO YOUR TASK AND THE ACTUAL STRUCTURE OF nvidia/OpenCodeReasoning**\n",
    "# This prompt needs to match the format your model was fine-tuned on.\n",
    "# The `preprocess_function` and Gemma's chat template (e.g. <start_of_turn>user...) are key here.\n",
    "\n",
    "# Let's assume your fine-tuning data looked like:\n",
    "# \"Instruction: [some instruction/description from nvidia/OpenCodeReasoning]\n",
    "Output: [corresponding code/solution]\"\n",
    "# For inference, you provide the \"Instruction\" part and let the model generate the \"Output\".\n",
    "\n",
    "# Example based on a hypothetical structure of OpenCodeReasoning:\n",
    "# This is a placeholder prompt. You need to replace it with a relevant prompt\n",
    "# based on the `nvidia/OpenCodeReasoning` dataset's expected input format.\n",
    "# For Gemma 1.1 IT, the prompt should follow its chat template.\n",
    "\n",
    "user_prompt_content = \"Write a Python function that calculates the factorial of a number.\" # Replace with actual prompt from dataset or a new one\n",
    "\n",
    "# Format the prompt according to Gemma's required template\n",
    "# This is crucial for instruction-tuned models.\n",
    "# The tokenizer's chat template can be helpful if your input fits a multi-turn chat.\n",
    "# For single-turn instruction following, a common format is:\n",
    "prompt_for_model = f\"<start_of_turn>user\\n{user_prompt_content}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "print(f\"Formatted prompt:\\n{prompt_for_model}\")\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(prompt_for_model, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "# Generate output\n",
    "# Adjust generation parameters as needed (max_new_tokens, temperature, top_p, etc.)\n",
    "print(\"\\nGenerating response...\")\n",
    "with torch.no_grad(): # Ensure no gradients are calculated during inference\n",
    "    outputs = ft_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,  # Adjust as needed\n",
    "        do_sample=True,      # Whether to use sampling; set to False for greedy decoding\n",
    "        temperature=0.7,     # Controls randomness. Lower is more deterministic. Only used if do_sample=True.\n",
    "        top_p=0.9,           # Nucleus sampling. Only used if do_sample=True.\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id # Important for generation\n",
    "    )\n",
    "\n",
    "# Decode the generated tokens\n",
    "# The output includes the prompt, so we decode and then can choose to print only the generated part.\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=False) # Set skip_special_tokens=True if you don't want to see them\n",
    "\n",
    "# Extract only the newly generated text (after the prompt)\n",
    "# This simple split assumes the prompt_for_model is exactly at the beginning of the response.\n",
    "# More robust parsing might be needed if the model adds tokens before echoing the prompt.\n",
    "generated_text = full_response.split(prompt_for_model)[-1] if prompt_for_model in full_response else full_response\n",
    "\n",
    "print(\"\\n--- Full Response (including prompt) ---\")\n",
    "print(full_response)\n",
    "print(\"\\n--- Generated Text Only ---\")\n",
    "# A common way to clean up the response is to stop at the next <end_of_turn> or eos_token if they appear in the generation\n",
    "if tokenizer.eos_token in generated_text:\n",
    "    generated_text = generated_text.split(tokenizer.eos_token)[0]\n",
    "if \"<end_of_turn>\" in generated_text: # Specific to Gemma's formatting\n",
    "    generated_text = generated_text.split(\"<end_of_turn>\")[0]\n",
    "\n",
    "print(generated_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusion and Next Steps\n",
    "\n",
    "This notebook provided a comprehensive walkthrough of fine-tuning a Gemma model for code-related tasks using the `nvidia/OpenCodeReasoning` dataset.\n",
    "\n",
    "**Key Takeaways:**\n",
    "*   **Data is King:** The success of fine-tuning heavily depends on the quality of your dataset and how well you preprocess it to match the model's expected input format. The `preprocess_function` is where you'll spend significant time adapting to `nvidia/OpenCodeReasoning`.\n",
    "*   **Prompt Formatting:** For instruction/chat models like Gemma, adhering to their specific prompt template (e.g., with `<start_of_turn>`, `<end_of_turn>`) is essential for good performance during both fine-tuning and inference.\n",
    "*   **Efficient Fine-Tuning:** Techniques like LoRA and 4-bit quantization make it possible to fine-tune large models on consumer-grade or free-tier cloud GPUs.\n",
    "\n",
    "**Potential Next Steps:**\n",
    "*   **Thoroughly Adapt `preprocess_function`:** Dive deep into the `nvidia/OpenCodeReasoning` dataset's structure. Identify the correct fields for your input (e.g., problem description, context) and target (e.g., code solution, explanation). Modify the `preprocess_function` in section 2.3 to correctly transform these fields into the Gemma instruction format.\n",
    "*   **Experiment with Hyperparameters:** Adjust learning rate, batch size, number of epochs, LoRA rank (`r`), and alpha (`lora_alpha`) to optimize performance.\n",
    "*   **Evaluation:** Implement a proper evaluation strategy using a held-out test set and relevant metrics for code generation/reasoning (e.g., BLEU, CodeBLEU, execution accuracy, pass@k). The `SFTTrainer` can take an `eval_dataset`.\n",
    "*   **Advanced Prompting:** Explore more sophisticated prompting techniques if the initial results are not satisfactory.\n",
    "*   **Larger Models:** If resources allow, experiment with larger Gemma variants (e.g., `gemma-1.1-7b-it`) for potentially better performance, adjusting quantization and batch sizes accordingly.\n",
    "*   **Packing:** For faster training, especially with sequences of varying lengths, explore sequence packing by setting `packing=True` in `SFTTrainer` and ensuring your dataset is formatted correctly for it.\n",
    "\n",
    "Good luck with your fine-tuning project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
